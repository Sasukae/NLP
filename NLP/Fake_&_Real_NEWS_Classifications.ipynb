{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a9796bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8809dd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['Unnamed: 0', 'label', 'text', 'label_num'], dtype='object'),\n",
       " ham     3672\n",
       " spam    1499\n",
       " Name: label, dtype: int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"spam_ham_dataset.csv\")\n",
    "data.columns, data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ebd2ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3672\n",
       "1    1499\n",
       "Name: label_num, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"label_num\"] = data.label.map({\"spam\" : 1,\"ham\":0})\n",
    "data[\"label_num\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9e863c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'text', 'label_num'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop('label',axis='columns', inplace = True)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d9be80",
   "metadata": {},
   "source": [
    "## This is clearly a class imbalance problem now we have to make 2 class ham and spam has same number of samples. This we can do in many ways\n",
    "1. Under Sampling:-  where we take minority class number of samples for majority class \"df_class_0_under = df_class_0.sample(count_class_1)\"\n",
    "2. Over Sampling:- Where we duplicate the minority class sample equal to majority class\"df_class_1_over = df_class_1.sample(count_class_0, replace=True)\"\n",
    "3. SMOTE:- SMOTE stands for Synthetic Minority Oversampling Technique: Using imbalance learn it uses knn to generate nearest sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1573e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SMOTE Example works at integer or float columns\n",
    "# from imblearn.over_sampling import SMOTE #pip install imbalanced-learn\n",
    "# X = data.drop('label_num',axis='columns')\n",
    "# y = data['label_num']\n",
    "# smote = SMOTE(sampling_strategy='minority')\n",
    "# X_sm, y_sm = smote.fit_resample(X, y)\n",
    "\n",
    "# y_sm.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad504528",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO Over sampling\n",
    "# Class count\n",
    "count_class_0, count_class_1 = data.label_num.value_counts()\n",
    "# Divide by class\n",
    "df_class_0 = data[data['label_num'] == 0]\n",
    "df_class_1 = data[data['label_num'] == 1]\n",
    "\n",
    "df_class_1_over = df_class_1.sample(count_class_0, replace=True)\n",
    "df_class_1_over.shape,df_class_0.shape\n",
    "data_balanced = pd.concat([df_class_0,df_class_1_over])\n",
    "data_balanced.shape,data_balanced.label_num.value_counts()\n",
    "data_balanced[\"Pre_processed_text\"] = data_balanced[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8345354d",
   "metadata": {},
   "source": [
    "## Using Spacy do preprocessing and Do some general processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61b4c50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "bindings = {\n",
    "    \"aren't\" : \"are not\",\n",
    "    \"can't\" : \"cannot\",\n",
    "    \"couldn't\" : \"could not\",\n",
    "    \"didn't\" : \"did not\",\n",
    "    \"doesn't\" : \"does not\",\n",
    "    \"don't\" : \"do not\",\n",
    "    \"hadn't\" : \"had not\",\n",
    "    \"hasn't\" : \"has not\",\n",
    "    \"haven't\" : \"have not\",\n",
    "    \"he'd\" : \"he would\",\n",
    "    \"he'll\" : \"he will\",\n",
    "    \"he's\" : \"he is\",\n",
    "    \"i'd\" : \"I would\",\n",
    "    \"i'll\" : \"I will\",\n",
    "    \"i'm\" : \"I am\",\n",
    "    \"isn't\" : \"is not\",\n",
    "    \"it's\" : \"it is\",\n",
    "    \"it'll\":\"it will\",\n",
    "    \"i've\" : \"I have\",\n",
    "    \"let's\" : \"let us\",\n",
    "    \"mightn't\" : \"might not\",\n",
    "    \"mustn't\" : \"must not\",\n",
    "    \"shan't\" : \"shall not\",\n",
    "    \"she'd\" : \"she would\",\n",
    "    \"she'll\" : \"she will\",\n",
    "    \"she's\" : \"she is\",\n",
    "    \"shouldn't\" : \"should not\",\n",
    "    \"that's\" : \"that is\",\n",
    "    \"there's\" : \"there is\",\n",
    "    \"they'd\" : \"they would\",\n",
    "    \"they'll\" : \"they will\",\n",
    "    \"they're\" : \"they are\",\n",
    "    \"they've\" : \"they have\",\n",
    "    \"we'd\" : \"we would\",\n",
    "    \"we're\" : \"we are\",\n",
    "    \"weren't\" : \"were not\",\n",
    "    \"we've\" : \"we have\",\n",
    "    \"what'll\" : \"what will\",\n",
    "    \"what're\" : \"what are\",\n",
    "    \"what's\" : \"what is\",\n",
    "    \"what've\" : \"what have\",\n",
    "    \"where's\" : \"where is\",\n",
    "    \"who'd\" : \"who would\",\n",
    "    \"who'll\" : \"who will\",\n",
    "    \"who're\" : \"who are\",\n",
    "    \"who's\" : \"who is\",\n",
    "    \"who've\" : \"who have\",\n",
    "    \"won't\" : \"will not\",\n",
    "    \"wouldn't\" : \"would not\",\n",
    "    \"you'd\" : \"you would\",\n",
    "    \"you'll\" : \"you will\",\n",
    "    \"you're\" : \"you are\",\n",
    "    \"you've\" : \"you have\",\n",
    "    \"'re\": \" are\",\n",
    "    \"let's\":\"let us\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'll\":\" will\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"wasnt\": \"was not\"\n",
    "}\n",
    "regex={\n",
    "    re.compile('|'.join([r'(\\$\\d*\\.\\d{1,2}\\+?)',r'(\\$\\d+\\+?)',r'(\\$\\d+\\.?\\+?)',r'(\\d+(?:[\\.,]\\d+)?dollars)',r'(\\d+(?:[\\.,]\\d+)? usd)', r'(\\d+(?:[\\.,]\\d+)?dollar)'])) : r\"dollar ||curr||\",\n",
    "\n",
    "    re.compile(r' \\$+ ', re.UNICODE) : r\" dollar \",\n",
    "    re.compile('|'.join([r'\\b\\d+ hours\\b', r'\\b\\d+ hrs\\b',r'\\b\\d+hour\\b', r'\\b\\d+ hr\\b',r'\\b\\d+-hours\\b', r'\\b\\d+-hrs\\b',r'\\b\\d+ -hours\\b', r'\\b\\d+ - hrs\\b'])) : r\"hours ||duration||\",\n",
    "\n",
    "    re.compile('|'.join([r'\\b\\d+ minutes\\b', r'\\b\\d+ mins\\b', r'\\b\\d+min\\b',r'\\b\\d+-minutes\\b', r'\\b\\d+-mins\\b', r'\\b\\d+-min\\b',r'\\b\\d+ -minutes\\b', r'\\b\\d+ - mins\\b', r'\\b\\d+ - min\\b'])) : r\"minutes||duration||\",\n",
    "\n",
    "    re.compile('|'.join([r'\\b\\d+ seconds\\b', r'\\b\\d+ secs\\b', r'\\b\\d+sec\\b',r'\\b\\d+-seconds\\b', r'\\b\\d+-secs\\b', r'\\b\\d+-sec\\b',r'\\b\\d+ -seconds\\b', r'\\b\\d+ - secs\\b', r'\\b\\d+ - sec\\b'])) : r\"seconds||duration||\",\n",
    "\n",
    "    re.compile(r'\\b\\d+:\\d+') : r\"||abstime||\"\n",
    "}\n",
    "\n",
    "def replaceBindings(text_df,col,bi_dict):\n",
    "    #Input: Data frame and text column and dict for replacement\n",
    "    #Output: Text column converted by replacing bindings\n",
    "    text_df[col]=text_df[col].replace(bi_dict,regex=True)\n",
    "    return text_df\n",
    "\n",
    "\n",
    "def replaceRegex(text_df,col,reg_dict):\n",
    "    #Input: Data frame and text column and dict for replacement\n",
    "    #Output: Text column converted by replacing regex\n",
    "    text_df[col]=text_df[col].replace(reg_dict,regex=True)\n",
    "    return text_df\n",
    "\n",
    "\n",
    "## Step1 : Remove unwanted text and special characters\n",
    "data_balanced['Pre_processed_text'] = data_balanced['Pre_processed_text'].apply(lambda x: x.replace(\"Subject:\",\"\"))\n",
    "data_balanced['Pre_processed_text'] = data_balanced['Pre_processed_text'].apply(lambda x: x.replace(\"\\r\\n\",\" \"))\n",
    "special_list = [\"%\",\"*\",'+','[',']',\"-\",\"=\",\"_\",\"|\",\"<\",\">\",\"@\",'/',\":\",\";\",\"#\"]\n",
    "for sc in special_list:\n",
    "    data_balanced['Pre_processed_text'] = data_balanced['Pre_processed_text'].apply(lambda x: x.replace(sc,\"\"))\n",
    "\n",
    "## Step2: Lower the text\n",
    "data_balanced['Pre_processed_text'] = data_balanced['Pre_processed_text'].apply(lambda x: x.lower())\n",
    "\n",
    "## Step3: Replace the Bindings\n",
    "data_balanced = replaceBindings(data_balanced,'Pre_processed_text',bindings)\n",
    "\n",
    "## Step4: Replace the Regex\n",
    "data_balanced = replaceRegex(data_balanced,'Pre_processed_text',regex)\n",
    "\n",
    "## Step 5 using spacy remove stop words punctuatutions and get lemmatization of words\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "def preprocessing(text):\n",
    "    doc = nlp(text) ### getting doc tokenize\n",
    "    nlp.vocab['not'].is_stop = \"False\"\n",
    "    req_token = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct: ## Get only token which are not stop words and punctuation\n",
    "            continue\n",
    "        req_token.append(token.lemma_) ## Get the lemmatization of a token\n",
    "    return \" \".join(req_token)\n",
    "\n",
    "data_balanced[\"Pre_processed_text\"] = data_balanced['Pre_processed_text'].apply(lambda x: preprocessing(x))\n",
    "data_balanced.to_csv(\"Pre_Processed_text.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae01460",
   "metadata": {},
   "source": [
    "## Try to develop a model which identify a news is fake or real. \n",
    "\n",
    "### In general text is converted to number vector in 5 ways \n",
    "1. label Encoding (Give the unique number to all the words in vocabulary and tag the documnet) not famous has lot of drawbacks. No meaning captured, data is sparse, for new word in test need to be treated as same.\n",
    "2. One Hot Encoding, one hot for all the words in vocab, not famous has lot of drawbacks. No meaning captured, data is sparse, for new word in test need to be treated as same.\n",
    "3. Bag of Words, Count the number of times word present in a documnet. less sparse dataset, still meaning between the words are not captured.\n",
    "4. TF-IDF: tf(total number of time term t present in doc A/ total number of terms in doc A) * idf(log(total number of doc present/number of documnets term t present)) ex:- 48/1000 * 4/3,4/1. As n increases dimensionlity increases, Doesn't capture relation between words, doesn't address out of vocabulary problem. \n",
    "5. Word embeddings(GLove(method -Continuos bag of words), Gensim(method -Continuos bag of words),FastText(method - Character n gram), Bert(Transformers).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181d36bc",
   "metadata": {},
   "source": [
    "## Using Bag of words and build a classification model With Preprocessing txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22bb7153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85e6aa1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5875,),\n",
       " 1    735\n",
       " 0    734\n",
       " Name: label_num, dtype: int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(data_balanced.Pre_processed_text,data_balanced.label_num, test_size = 0.2,random_state=2023, stratify=data_balanced.label_num)\n",
    "X_train.shape,y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fec50ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  {'one': 2, 'praneeth': 3, 'helps': 1, 'two': 4, 'friends': 0}\n",
      "Vocabulary:  {'one': 3, 'praneeth': 5, 'helps': 1, 'two': 7, 'friends': 0, 'one praneeth': 4, 'praneeth helps': 6, 'helps two': 2, 'two friends': 8}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## example of CountVectorizer\n",
    "document = [\"One praneeth helps Two friends\"]\n",
    "\n",
    "# Create a Vectorizer Object where ngram = 1\n",
    "vectorizer = CountVectorizer()\n",
    " \n",
    "vectorizer.fit(document)\n",
    "# Printing the identified Unique words along with their indices\n",
    "print(\"Vocabulary: \", vectorizer.vocabulary_)\n",
    "\n",
    "vector = vectorizer.transform(document)\n",
    "vector.toarray()\n",
    "\n",
    "# n-gram =2 \n",
    "vectorizer = CountVectorizer(ngram_range = (1,2))\n",
    "vectorizer.fit(document)\n",
    "# Printing the identified Unique words along with their indices\n",
    "print(\"Vocabulary: \", vectorizer.vocabulary_)\n",
    "\n",
    "vector = vectorizer.transform(document)\n",
    "vector.toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6b87b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classifier with bag of words ngram = 1: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       734\n",
      "           1       0.99      0.96      0.97       735\n",
      "\n",
      "    accuracy                           0.97      1469\n",
      "   macro avg       0.97      0.97      0.97      1469\n",
      "weighted avg       0.97      0.97      0.97      1469\n",
      "\n",
      "KNN Classifier with bag of words ngram = 1,2,3 : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       734\n",
      "           1       0.99      0.95      0.97       735\n",
      "\n",
      "    accuracy                           0.97      1469\n",
      "   macro avg       0.97      0.97      0.97      1469\n",
      "weighted avg       0.97      0.97      0.97      1469\n",
      "\n",
      "KNN Classifier with bag of words ngram = 3 : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96       734\n",
      "           1       0.93      1.00      0.96       735\n",
      "\n",
      "    accuracy                           0.96      1469\n",
      "   macro avg       0.96      0.96      0.96      1469\n",
      "weighted avg       0.96      0.96      0.96      1469\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Build a pipeline wich will do vectorizer with ngram = 1 and model\n",
    "Kn_pipeline= Pipeline([\n",
    "    (\"vectorizer\",CountVectorizer()),\n",
    "    (\"model\",KNeighborsClassifier(n_neighbors = 2 , metric = \"cosine\"))\n",
    "])\n",
    "Kn_pipeline.fit(X_train,y_train)\n",
    "y_pred = Kn_pipeline.predict(X_test)\n",
    "print(\"KNN Classifier with bag of words ngram = 1: \\n {}\".format(classification_report(y_test,y_pred)))\n",
    "\n",
    "## Build a pipeline wich will do vectorizer with ngram = 1,2,3 and model\n",
    "Kn_pipeline= Pipeline([\n",
    "    (\"vectorizer\",CountVectorizer(ngram_range = (1,3))),\n",
    "    (\"model\",KNeighborsClassifier(n_neighbors = 2 , metric = \"cosine\"))\n",
    "])\n",
    "Kn_pipeline.fit(X_train,y_train)\n",
    "y_pred = Kn_pipeline.predict(X_test)\n",
    "print(\"KNN Classifier with bag of words ngram = 1,2,3 : \\n {}\".format(classification_report(y_test,y_pred)))\n",
    "\n",
    "## Build a pipeline wich will do vectorizer with ngram = 3 and model\n",
    "Kn_pipeline= Pipeline([\n",
    "    (\"vectorizer\",CountVectorizer(ngram_range = (3,3))),\n",
    "    (\"model\",KNeighborsClassifier(n_neighbors = 2 , metric = \"cosine\"))\n",
    "])\n",
    "Kn_pipeline.fit(X_train,y_train)\n",
    "y_pred = Kn_pipeline.predict(X_test)\n",
    "print(\"KNN Classifier with bag of words ngram = 3 : \\n {}\".format(classification_report(y_test,y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa525835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Classifier with bag of words ngram = 1: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       734\n",
      "           1       0.98      1.00      0.99       735\n",
      "\n",
      "    accuracy                           0.99      1469\n",
      "   macro avg       0.99      0.99      0.99      1469\n",
      "weighted avg       0.99      0.99      0.99      1469\n",
      "\n",
      "RF Classifier with bag of words ngram = 1,2,3 : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       734\n",
      "           1       0.96      1.00      0.98       735\n",
      "\n",
      "    accuracy                           0.98      1469\n",
      "   macro avg       0.98      0.98      0.98      1469\n",
      "weighted avg       0.98      0.98      0.98      1469\n",
      "\n",
      "RF Classifier with bag of words ngram = 3 : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.69      0.82       734\n",
      "           1       0.76      1.00      0.87       735\n",
      "\n",
      "    accuracy                           0.85      1469\n",
      "   macro avg       0.88      0.85      0.84      1469\n",
      "weighted avg       0.88      0.85      0.84      1469\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Build a pipeline wich will do vectorizer with ngram = 1,2,3 and model Random Forest\n",
    "\n",
    "RF_pipeline= Pipeline([\n",
    "    (\"vectorizer\",CountVectorizer()),\n",
    "    (\"model\",RandomForestClassifier(n_estimators = 100, criterion = \"entropy\"))\n",
    "])\n",
    "RF_pipeline.fit(X_train,y_train)\n",
    "y_pred = RF_pipeline.predict(X_test)\n",
    "print(\"RF Classifier with bag of words ngram = 1: \\n {}\".format(classification_report(y_test,y_pred)))\n",
    "\n",
    "## Build a pipeline wich will do vectorizer with ngram = 1,2,3 and model\n",
    "RF_pipeline= Pipeline([\n",
    "    (\"vectorizer\",CountVectorizer(ngram_range = (1,3))),\n",
    "    (\"model\",RandomForestClassifier(n_estimators = 100, criterion = \"entropy\"))\n",
    "])\n",
    "RF_pipeline.fit(X_train,y_train)\n",
    "y_pred = RF_pipeline.predict(X_test)\n",
    "print(\"RF Classifier with bag of words ngram = 1,2,3 : \\n {}\".format(classification_report(y_test,y_pred)))\n",
    "\n",
    "## Build a pipeline wich will do vectorizer with ngram = 3 and model\n",
    "RF_pipeline= Pipeline([\n",
    "    (\"vectorizer\",CountVectorizer(ngram_range = (3,3))),\n",
    "    (\"model\",RandomForestClassifier(n_estimators = 100, criterion = \"entropy\"))\n",
    "])\n",
    "RF_pipeline.fit(X_train,y_train)\n",
    "y_pred = RF_pipeline.predict(X_test)\n",
    "print(\"RF Classifier with bag of words ngram = 3 : \\n {}\".format(classification_report(y_test,y_pred)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1189caa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Classifier with bag of words ngram = 1 : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       734\n",
      "           1       0.97      0.99      0.98       735\n",
      "\n",
      "    accuracy                           0.98      1469\n",
      "   macro avg       0.98      0.98      0.98      1469\n",
      "weighted avg       0.98      0.98      0.98      1469\n",
      "\n",
      "NB Classifier with bag of words ngram = 1,2,3 : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       734\n",
      "           1       0.99      0.99      0.99       735\n",
      "\n",
      "    accuracy                           0.99      1469\n",
      "   macro avg       0.99      0.99      0.99      1469\n",
      "weighted avg       0.99      0.99      0.99      1469\n",
      "\n",
      "NB Classifier with bag of words ngram = 3 : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       734\n",
      "           1       1.00      0.94      0.97       735\n",
      "\n",
      "    accuracy                           0.97      1469\n",
      "   macro avg       0.97      0.97      0.97      1469\n",
      "weighted avg       0.97      0.97      0.97      1469\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "## Build a pipeline wich will do vectorizer with ngram = 1 and model\n",
    "NB_pipeline= Pipeline([\n",
    "    (\"vectorizer\",CountVectorizer()),\n",
    "    (\"model\",MultinomialNB(alpha = 0.75))\n",
    "])\n",
    "NB_pipeline.fit(X_train,y_train)\n",
    "y_pred = NB_pipeline.predict(X_test)\n",
    "print(\"NB Classifier with bag of words ngram = 1 : \\n {}\".format(classification_report(y_test,y_pred)))\n",
    "\n",
    "## Build a pipeline wich will do vectorizer with ngram = 1,2,3 and model\n",
    "NB_pipeline= Pipeline([\n",
    "    (\"vectorizer\",CountVectorizer(ngram_range = (1,3))),\n",
    "    (\"model\",MultinomialNB(alpha = 0.75))\n",
    "])\n",
    "NB_pipeline.fit(X_train,y_train)\n",
    "y_pred = NB_pipeline.predict(X_test)\n",
    "print(\"NB Classifier with bag of words ngram = 1,2,3 : \\n {}\".format(classification_report(y_test,y_pred)))\n",
    "\n",
    "## Build a pipeline wich will do vectorizer with ngram = 3 and model\n",
    "NB_pipeline= Pipeline([\n",
    "    (\"vectorizer\",CountVectorizer(ngram_range = (3,3))),\n",
    "    (\"model\",MultinomialNB(alpha = 0.75))\n",
    "])\n",
    "NB_pipeline.fit(X_train,y_train)\n",
    "y_pred = NB_pipeline.predict(X_test)\n",
    "print(\"NB Classifier with bag of words ngram = 3 : \\n {}\".format(classification_report(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fe1811",
   "metadata": {},
   "source": [
    "## Now try to generate word 2 vector using TFIDF Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "174d036d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classifier with TFIDF: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98       734\n",
      "           1       0.99      0.96      0.98       735\n",
      "\n",
      "    accuracy                           0.98      1469\n",
      "   macro avg       0.98      0.98      0.98      1469\n",
      "weighted avg       0.98      0.98      0.98      1469\n",
      "\n",
      "RF Classifier with TFIDF: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99       734\n",
      "           1       0.97      1.00      0.99       735\n",
      "\n",
      "    accuracy                           0.99      1469\n",
      "   macro avg       0.99      0.99      0.99      1469\n",
      "weighted avg       0.99      0.99      0.99      1469\n",
      "\n",
      "NB Classifier with TFIDF: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       734\n",
      "           1       0.97      0.98      0.97       735\n",
      "\n",
      "    accuracy                           0.97      1469\n",
      "   macro avg       0.97      0.97      0.97      1469\n",
      "weighted avg       0.97      0.97      0.97      1469\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "## Build a pipeline wich will do vectorizer and model  KNNN\n",
    "KN_pipeline= Pipeline([\n",
    "    (\"vectorizer\",TfidfVectorizer()),\n",
    "    (\"model\",KNeighborsClassifier(n_neighbors = 2 , metric = \"cosine\"))\n",
    "])\n",
    "KN_pipeline.fit(X_train,y_train)\n",
    "y_pred = KN_pipeline.predict(X_test)\n",
    "print(\"KNN Classifier with TFIDF: \\n {}\".format(classification_report(y_test,y_pred)))\n",
    "\n",
    "## Build a pipeline wich will do vectorizer with ngram = 1,2,3 and model RF\n",
    "RF_pipeline= Pipeline([\n",
    "    (\"vectorizer\",TfidfVectorizer()),\n",
    "    (\"model\",RandomForestClassifier(n_estimators = 100, criterion = \"entropy\"))\n",
    "])\n",
    "RF_pipeline.fit(X_train,y_train)\n",
    "y_pred = RF_pipeline.predict(X_test)\n",
    "print(\"RF Classifier with TFIDF: \\n {}\".format(classification_report(y_test,y_pred)))\n",
    "\n",
    "## Build a pipeline wich will do vectorizer with ngram = 3 and model Navie Bayes\n",
    "NB_pipeline= Pipeline([\n",
    "    (\"vectorizer\",TfidfVectorizer()),\n",
    "    (\"model\",MultinomialNB(alpha = 0.75))\n",
    "])\n",
    "NB_pipeline.fit(X_train,y_train)\n",
    "y_pred = NB_pipeline.predict(X_test)\n",
    "print(\"NB Classifier with TFIDF: \\n {}\".format(classification_report(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa74e71",
   "metadata": {},
   "source": [
    "## Word to Vectors Using Spacy pre trained model (GLOVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ddd6c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "      <th>Pre_processed_text</th>\n",
       "      <th>Text_vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>605</td>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>enron methanol   meter    988291 follow note...</td>\n",
       "      <td>[-0.116396755, -0.01640397, -1.3500654, 1.1144...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2349</td>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>0</td>\n",
       "      <td>hpl nom january 9 2001 attached file   hplno...</td>\n",
       "      <td>[0.4305962, -3.5029912, -0.22538322, 0.4564125...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3624</td>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>0</td>\n",
       "      <td>neon retreat ho ho ho wonderful time year   ...</td>\n",
       "      <td>[0.15420337, 0.414349, -1.8427932, -0.23462386...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2030</td>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "      <td>0</td>\n",
       "      <td>indian spring deal book teco pvr revenue u...</td>\n",
       "      <td>[0.40145546, 0.8645684, -2.441826, -0.648139, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2949</td>\n",
       "      <td>Subject: ehronline web address change\\r\\nthis ...</td>\n",
       "      <td>0</td>\n",
       "      <td>ehronline web address change message intend ...</td>\n",
       "      <td>[1.1535546, 0.14871399, 1.3144618, -0.7234356,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  label_num  \\\n",
       "0         605  Subject: enron methanol ; meter # : 988291\\r\\n...          0   \n",
       "1        2349  Subject: hpl nom for january 9 , 2001\\r\\n( see...          0   \n",
       "2        3624  Subject: neon retreat\\r\\nho ho ho , we ' re ar...          0   \n",
       "4        2030  Subject: re : indian springs\\r\\nthis deal is t...          0   \n",
       "5        2949  Subject: ehronline web address change\\r\\nthis ...          0   \n",
       "\n",
       "                                  Pre_processed_text  \\\n",
       "0    enron methanol   meter    988291 follow note...   \n",
       "1    hpl nom january 9 2001 attached file   hplno...   \n",
       "2    neon retreat ho ho ho wonderful time year   ...   \n",
       "4      indian spring deal book teco pvr revenue u...   \n",
       "5    ehronline web address change message intend ...   \n",
       "\n",
       "                                        Text_vectors  \n",
       "0  [-0.116396755, -0.01640397, -1.3500654, 1.1144...  \n",
       "1  [0.4305962, -3.5029912, -0.22538322, 0.4564125...  \n",
       "2  [0.15420337, 0.414349, -1.8427932, -0.23462386...  \n",
       "4  [0.40145546, 0.8645684, -2.441826, -0.648139, ...  \n",
       "5  [1.1535546, 0.14871399, 1.3144618, -0.7234356,...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_balanced[\"Text_vectors\"] = data_balanced[\"Pre_processed_text\"].apply(lambda x: nlp(x).vector)\n",
    "data_balanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1554f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5875,),\n",
       " 1    735\n",
       " 0    734\n",
       " Name: label_num, dtype: int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(data_balanced.Text_vectors,data_balanced.label_num, test_size = 0.2,random_state=2023, stratify=data_balanced.label_num)\n",
    "X_train.shape,y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f1bd124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2df2c2ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape\n",
    "X_train_stack = np.stack(X_train)\n",
    "X_train_stack[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "180ecbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44210124, 0.46387514, 0.45495138, ..., 0.3466497 , 0.4272204 ,\n",
       "        0.5262414 ],\n",
       "       [0.28936383, 0.5348895 , 0.4673059 , ..., 0.37713078, 0.56919205,\n",
       "        0.5240829 ],\n",
       "       [0.3544381 , 0.40016073, 0.49442762, ..., 0.31481326, 0.56105137,\n",
       "        0.47268748],\n",
       "       ...,\n",
       "       [0.31870535, 0.53391814, 0.34457618, ..., 0.4225276 , 0.45700148,\n",
       "        0.52994436],\n",
       "       [0.41558844, 0.5996464 , 0.3784262 , ..., 0.266191  , 0.48799622,\n",
       "        0.49995637],\n",
       "       [0.25866765, 0.7273883 , 0.41262874, ..., 0.51113445, 0.4243504 ,\n",
       "        0.4788338 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_stack = np.stack(X_train)\n",
    "X_test_stack = np.stack(X_test)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_stack_scald = scaler.fit_transform(X_train_stack)\n",
    "X_test_stack_scald = scaler.transform(X_test_stack)\n",
    "X_test_stack_scald"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0f1a21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classifier with sapcy w2v: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97       734\n",
      "           1       0.96      0.97      0.97       735\n",
      "\n",
      "    accuracy                           0.97      1469\n",
      "   macro avg       0.97      0.97      0.97      1469\n",
      "weighted avg       0.97      0.97      0.97      1469\n",
      "\n",
      "RF Classifier with sapcy w2v: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97       734\n",
      "           1       0.96      0.99      0.97       735\n",
      "\n",
      "    accuracy                           0.97      1469\n",
      "   macro avg       0.97      0.97      0.97      1469\n",
      "weighted avg       0.97      0.97      0.97      1469\n",
      "\n",
      "NB Classifier with sapcy w2v: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.71      0.74       734\n",
      "           1       0.73      0.80      0.77       735\n",
      "\n",
      "    accuracy                           0.75      1469\n",
      "   macro avg       0.76      0.75      0.75      1469\n",
      "weighted avg       0.76      0.75      0.75      1469\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Build a pipeline wich will do vectorizer and model  KNNN\n",
    "KN_pipeline= Pipeline([\n",
    "    \n",
    "    (\"model\",KNeighborsClassifier(n_neighbors = 2 , metric = \"cosine\"))\n",
    "])\n",
    "KN_pipeline.fit(X_train_stack_scald,y_train)\n",
    "y_pred = KN_pipeline.predict(X_test_stack_scald)\n",
    "print(\"KNN Classifier with sapcy w2v: \\n {}\".format(classification_report(y_test,y_pred)))\n",
    "\n",
    "## Build a pipeline wich will do vectorizer with ngram = 1,2,3 and model RF\n",
    "RF_pipeline= Pipeline([\n",
    "\n",
    "    (\"model\",RandomForestClassifier(n_estimators = 100, criterion = \"entropy\"))\n",
    "])\n",
    "RF_pipeline.fit(X_train_stack_scald,y_train)\n",
    "y_pred = RF_pipeline.predict(X_test_stack_scald)\n",
    "print(\"RF Classifier with sapcy w2v: \\n {}\".format(classification_report(y_test,y_pred)))\n",
    "\n",
    "## Build a pipeline wich will do vectorizer with ngram = 3 and model Navie Bayes\n",
    "NB_pipeline= Pipeline([\n",
    "\n",
    "    (\"model\",MultinomialNB(alpha = 0.75))\n",
    "])\n",
    "NB_pipeline.fit(X_train_stack_scald,y_train)\n",
    "y_pred = NB_pipeline.predict(X_test_stack_scald)\n",
    "print(\"NB Classifier with sapcy w2v: \\n {}\".format(classification_report(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8d6920",
   "metadata": {},
   "source": [
    "## Using Gensim Get Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8819329f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91868\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:173: UserWarning: A NumPy version >=1.19.5 and <1.27.0 is required for this version of SciPy (detected version 1.19.1)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882edc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\") # if this fails then run \"python -m spacy download en_core_web_lg\" to download that model\n",
    "\n",
    "def preprocess_and_vectorize(text):\n",
    "    # remove stop words and lemmatize the text\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        filtered_tokens.append(token.lemma_)\n",
    "        \n",
    "    return wv.get_mean_vector(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f73d193",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_balanced[\"Gensim_vectors\"] = data_balanced[\"Pre_processed_text\"].apply(lambda x: preprocess_and_vectorize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccf30b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train before reshaping:  (5875,)\n",
      "Shape of X_test before reshaping:  (1469,)\n",
      "Shape of X_train after reshaping:  (5875, 300)\n",
      "Shape of X_test after reshaping:  (1469, 300)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Do the 'train-test' splitting with test size of 20% with random state of 2022 and stratify sampling too\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_balanced.Gensim_vectors.values, \n",
    "    data_balanced.label_num, \n",
    "    test_size=0.2, # 20% samples will go to test dataset\n",
    "    random_state=2023,\n",
    "    stratify=data_balanced.label_num\n",
    ")\n",
    "\n",
    "print(\"Shape of X_train before reshaping: \", X_train.shape)\n",
    "print(\"Shape of X_test before reshaping: \", X_test.shape)\n",
    "\n",
    "\n",
    "X_train_2d = np.stack(X_train)\n",
    "X_test_2d =  np.stack(X_test)\n",
    "\n",
    "print(\"Shape of X_train after reshaping: \", X_train_2d.shape)\n",
    "print(\"Shape of X_test after reshaping: \", X_test_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "944c68ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classifier with genism w2v: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       734\n",
      "           1       0.98      0.97      0.98       735\n",
      "\n",
      "    accuracy                           0.98      1469\n",
      "   macro avg       0.98      0.98      0.98      1469\n",
      "weighted avg       0.98      0.98      0.98      1469\n",
      "\n",
      "RF Classifier with genism w2v: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       734\n",
      "           1       0.98      0.99      0.98       735\n",
      "\n",
      "    accuracy                           0.98      1469\n",
      "   macro avg       0.98      0.98      0.98      1469\n",
      "weighted avg       0.98      0.98      0.98      1469\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Build a pipeline wich will do vectorizer and model  KNNN\n",
    "KN_pipeline= Pipeline([\n",
    "    \n",
    "    (\"model\",KNeighborsClassifier(n_neighbors = 2 , metric = \"cosine\"))\n",
    "])\n",
    "KN_pipeline.fit(X_train_2d,y_train)\n",
    "y_pred = KN_pipeline.predict(X_test_2d)\n",
    "print(\"KNN Classifier with genism w2v: \\n {}\".format(classification_report(y_test,y_pred)))\n",
    "\n",
    "## Build a pipeline wich will do vectorizer with ngram = 1,2,3 and model RF\n",
    "RF_pipeline= Pipeline([\n",
    "\n",
    "    (\"model\",RandomForestClassifier(n_estimators = 100, criterion = \"entropy\"))\n",
    "])\n",
    "RF_pipeline.fit(X_train_2d,y_train)\n",
    "y_pred = RF_pipeline.predict(X_test_2d)\n",
    "print(\"RF Classifier with genism w2v: \\n {}\".format(classification_report(y_test,y_pred)))\n",
    "\n",
    "## Build a pipeline wich will do vectorizer with ngram = 3 and model Navie Bayes\n",
    "# NB_pipeline= Pipeline([\n",
    "\n",
    "#     (\"model\",MultinomialNB(alpha = 0.75))\n",
    "# ])\n",
    "# NB_pipeline.fit(X_train_2d,y_train)\n",
    "# y_pred = NB_pipeline.predict(X_test_2d)\n",
    "# print(\"NB Classifier with genism w2v: \\n {}\".format(classification_report(y_test,y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
